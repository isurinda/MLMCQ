<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
<title>Interactive MCQ Quiz ‚Äì 99 Questions</title>
<style>
  :root { --fg:#0f172a; --muted:#475569; --border:#e2e8f0; --ok:#16a34a; --bad:#dc2626; }
  html,body { background:#fff; color:var(--fg); font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Arial, 'Helvetica Neue', sans-serif; }
  body { margin:0; padding:16px; max-width:900px; margin-inline:auto; line-height:1.5; }
  h1 { font-size:clamp(22px, 4vw, 32px); margin: 8px 0 4px; }
  .meta { color:var(--muted); margin-bottom: 12px; }
  .controls { position: sticky; top: 0; background: #fff; padding: 12px 0; border-bottom: 1px solid var(--border); z-index: 20; display: flex; gap: 8px; align-items: center; flex-wrap: wrap; }
  button, select { -webkit-tap-highlight-color: transparent; }
  .btn { border:1px solid var(--border); padding:10px 14px; border-radius:12px; background:#fff; font-weight:600; }
  .btn:active { transform: scale(0.99); }
  .badge { border-radius: 999px; padding:6px 12px; background:#f1f5f9; font-weight:600; }
  .qcard { border:1px solid var(--border); border-radius:16px; padding:16px; margin:14px 0; box-shadow:0 1px 2px rgba(0,0,0,.03); }
  .qtitle { font-weight:700; margin-bottom:8px; }
  .choices label { display:flex; gap:10px; padding:12px; border-radius:10px; border:1px solid transparent; }
  .choices input { inline-size:20px; block-size:20px; margin-top:2px; }
  .choice-ok { background:#ecfdf5; border-color:#86efac; }
  .choice-bad { background:#fef2f2; border-color:#fecaca; }
  .result { font-weight:700; margin-top:8px; }
  .res-ok { color:var(--ok); } .res-bad { color: var(--bad);}
  .explain { background:#f8fafc; border-left:4px solid #3b82f6; padding:12px; border-radius:8px; margin-top:10px; display:none; }
  .tools { display:flex; gap:8px; align-items:center; flex-wrap:wrap; }
  .small { font-size:12px; color:var(--muted); }
  .editbox { display:none; margin-top:8px; gap:8px; align-items:center; flex-wrap:wrap; }
  .editbox select, .editbox textarea { width:100%; max-width:100%; padding:10px; border:1px solid var(--border); border-radius:10px; font: inherit; }
  @media (hover:none) { .choices label:active { transform: scale(0.99); } }
</style>
</head>
<body>
  <h1>Interactive MCQ Quiz</h1>
  <div class="meta">Machine Learning & EDA ‚Äî 99 Questions</div>

  <div class="controls" role="region" aria-label="Quiz controls">
    <button class="btn" id="showScore">üìä Show score</button>
    <button class="btn" id="reset">üîÑ Reset</button>
    <button class="btn" id="export">‚¨áÔ∏è Export progress</button>
    <input type="file" id="import" accept="application/json" style="display:none">
    <button class="btn" id="importBtn">‚¨ÜÔ∏è Import</button>
    <span class="badge" id="progress">0 / 0 answered</span>
    <span class="badge" id="score">Score: 0</span>
  </div>

  <div id="quiz"></div>

<script>
const DATA = [{"qnum": 1, "question": "What is the primary advantage of hot-deck imputation over mean imputation?", "options": ["It reduces data completeness", "It increases data consistency", "It preserves data structure and relationships", "It is faster and computationally efficient"], "correct": 2, "explanation": "Hot-deck imputation works by finding a 'similar' record (based on other attributes) and using its value to fill the missing data. This approach maintains the natural relationships between variables in your dataset. For example, if age and income are correlated, hot-deck ensures the imputed income matches what someone of that age typically earns. In contrast, mean imputation simply plugs in the average value, which artificially reduces variance and can destroy correlations between variables, leading to biased statistical analyses."}, {"qnum": 2, "question": "Mean imputation for handling missing data involves:", "options": ["Replacing missing values with the median of the variable", "Filling missing values with the mode of the variable", "Replacing missing values with the mean of the variable", "Removing the rows with missing values"], "correct": 2, "explanation": "Mean imputation is one of the simplest techniques for handling missing data. It calculates the arithmetic mean (average) of all available values for a particular variable and uses that mean to fill in any missing entries. For example, if you have ages: 25, 30, ?, 35, 40, the missing value would be replaced with (25+30+35+40)/4 = 32.5. While simple to implement, this method has limitations as it doesn't account for relationships with other variables and artificially reduces variability in the data."}, {"qnum": 3, "question": "Which phase of CRISP-DM involves constructing the predictive model using a suitable algorithm?", "options": ["Data Understanding", "Data Preparation", "Modeling", "Evaluation"], "correct": 2, "explanation": "The Modeling phase is where the actual machine learning happens. During this phase, you select appropriate algorithms (like decision trees, neural networks, or regression models), apply them to your prepared data, and train the models. This phase also includes parameter tuning and optimization to improve model performance. Think of it as the 'construction' phase where you build and refine your predictive tool using various modeling techniques and iteratively improve their accuracy."}, {"qnum": 4, "question": "Which aspect is crucial for data quality assurance?", "options": ["Data irrelevance", "Data consistency", "Data duplication", "Data variety"], "correct": 1, "explanation": "Data consistency is fundamental to quality assurance because it ensures that data values don't contradict each other across different sources, time periods, or within the same dataset. For example, if a customer's age is recorded as 25 in one table but 52 in another, this inconsistency creates reliability issues. Consistent data means the same information is represented uniformly throughout your system, making it trustworthy and suitable for analysis and decision-making. The other options (irrelevance, duplication, variety) are either problems to avoid or simply characteristics rather than quality dimensions."}, {"qnum": 5, "question": "Which of the following is a potential drawback of listwise deletion?", "options": ["It increases data completeness", "It reduces data accuracy", "It can lead to a loss of valuable data and information", "It is computationally intensive"], "correct": 2, "explanation": "Listwise deletion (also called complete case analysis) removes entire rows/records that have any missing values. The major drawback is that you might throw away a lot of useful information. For example, if you have a dataset with 1000 rows and 20 columns, and each row is missing just one value in different columns, you could end up deleting all 1000 rows even though 95% of your data is actually present! This drastic reduction in sample size can lead to biased results and reduced statistical power, especially if the missing data isn't random."}, {"qnum": 6, "question": "What does CRISP-DM stand for?", "options": ["Comprehensive Reporting In Software Processing for Data Mining", "Cross-industry Standard Process for Data Mining", "Coordinated Resource Integration for Software Processing in Data Mining", "Complex Regression and Iterative Strategy for Processing Data in Mining"], "correct": 1, "explanation": "CRISP-DM stands for Cross-Industry Standard Process for Data Mining. Developed in the late 1990s, it's the most widely used methodology for data mining and analytics projects across all industries. The 'cross-industry' part is important because it means the framework is not specific to any particular sector‚Äîit works equally well for finance, healthcare, retail, manufacturing, and any other field. It provides a structured approach with six phases: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment."}, {"qnum": 7, "question": "Handling noisy and imperfect data involves:", "options": ["Increasing the complexity of the data", "Eliminating outliers from the dataset", "Accepting the data as it is, without any modifications", "Cleaning and filtering the data to remove inconsistencies"], "correct": 3, "explanation": "Noisy data contains errors, inconsistencies, or random variations that don't represent the true signal. Proper handling requires systematic cleaning and filtering‚Äîidentifying and correcting errors, standardizing formats, removing duplicate entries, and addressing inconsistencies. This might include spell-checking text fields, validating data ranges, or smoothing anomalous values. Simply accepting noisy data leads to poor model performance, while blindly eliminating all outliers can remove valuable information. The goal is targeted cleaning based on understanding what represents noise versus legitimate variation in your specific context."}, {"qnum": 8, "question": "What is the purpose of the \"Evaluation\" phase in CRISP-DM?", "options": ["To build the machine learning model", "To assess the performance of the model and validate it against the business objectives", "To collect the data", "To prepare the data for analysis"], "correct": 1, "explanation": "The Evaluation phase is where you take a critical look at your model before deploying it. This involves two key aspects: (1) Technical evaluation‚Äîmeasuring metrics like accuracy, precision, recall, F1-score, or RMSE to see how well the model performs on test data, and (2) Business evaluation‚Äîdetermining whether the model actually solves the business problem defined in the Business Understanding phase. A technically accurate model that doesn't meet business needs is still a failure. This phase might reveal that you need to return to earlier stages to refine your approach."}, {"qnum": 9, "question": "In CRISP-DM, the \"Deployment\" phase includes:", "options": ["Fine-tuning the model", "Putting the model into practical use", "Data cleaning and preprocessing", "Collecting more data for analysis"], "correct": 1, "explanation": "Deployment is where your data mining work transitions from a project into operational reality. This phase involves integrating the model into existing business processes and systems where it can generate real value. Deployment might mean creating an API for real-time predictions, scheduling batch scoring jobs, building dashboards for stakeholders, or automating decision-making processes. It also includes creating documentation, training users, setting up monitoring systems, and planning for ongoing maintenance. A model sitting on a data scientist's laptop has zero business impact‚Äîdeployment is what makes it useful."}, {"qnum": 10, "question": "Which phase of CRISP-DM involves understanding the project objectives and requirements from a business perspective?", "options": ["Data Understanding", "Business Understanding", "Data Preparation", "Modeling"], "correct": 1, "explanation": "Business Understanding is the critical first phase where you establish the foundation for the entire project. This involves meeting with stakeholders to understand their goals, defining what success looks like, identifying constraints (budget, timeline, resources), and translating business problems into data mining objectives. For example, 'reduce customer churn' might translate to 'build a classification model to predict which customers are likely to leave in the next 90 days.' Without this phase, you risk building technically impressive models that solve the wrong problem or deliver insights no one can act upon."}, {"qnum": 11, "question": "Which aspect of machine learning deals with safeguarding data from unauthorised access and misuse?", "options": ["Data transparency", "Data privacy", "Data accuracy", "Data quantity"], "correct": 1, "explanation": "Data privacy encompasses all the policies, technologies, and practices designed to protect sensitive information from unauthorized access and misuse. In machine learning, this includes techniques like data anonymization (removing personally identifiable information), encryption (securing data in transit and at rest), access controls (ensuring only authorized individuals can access data), and compliance with regulations like GDPR or HIPAA. Privacy is especially critical in ML because models trained on sensitive data might inadvertently leak that information through their predictions or parameters if not properly protected."}, {"qnum": 12, "question": "Which type of Machine Learning involves learning from labeled data to make predictions or classifications?", "options": ["Supervised learning", "Unsupervised learning", "Reinforcement learning", "Semi-supervised learning"], "correct": 0, "explanation": "Supervised learning is like learning with a teacher who provides correct answers. You train the model using labeled data‚Äîexamples where you already know the correct output. For instance, to build a spam email detector, you'd provide thousands of emails labeled as 'spam' or 'not spam.' The model learns patterns that distinguish spam from legitimate emails, then uses these patterns to classify new, unseen emails. This 'supervision' (having labeled examples) allows the model to learn the mapping from inputs (features) to outputs (labels) and make accurate predictions on new data."}, {"qnum": 13, "question": "Which branch of machine learning explores the relationship between data points?", "options": ["Reinforcement learning", "Supervised learning", "Unsupervised learning", "Deep learning"], "correct": 2, "explanation": "Unsupervised learning works with unlabeled data to discover hidden patterns, structures, and relationships. Without predetermined categories or answers, these algorithms explore how data points relate to each other. Common tasks include clustering (grouping similar customers together for market segmentation), dimensionality reduction (finding which features are most important), and association rule mining (discovering that customers who buy product A often buy product B). Think of it as exploratory analysis‚Äîyou don't tell the algorithm what to find, you let it discover interesting patterns on its own."}, {"qnum": 14, "question": "Which type of Machine Learning is often used in autonomous robotics and game playing?", "options": ["Supervised learning", "Unsupervised learning", "Reinforcement learning", "Semi-supervised learning"], "correct": 2, "explanation": "Reinforcement learning (RL) is fundamentally different from supervised and unsupervised learning. An RL agent learns by interacting with an environment, taking actions, and receiving feedback in the form of rewards (for good actions) or penalties (for bad ones). The agent's goal is to learn a policy‚Äîa strategy for choosing actions‚Äîthat maximizes cumulative reward over time. It's like training a dog: you don't show examples of perfect behavior (supervised learning) or let it find patterns in data (unsupervised learning); instead, you reward good behavior and discourage bad behavior, and the dog learns through experience. RL powers game-playing AI like AlphaGo, robotics controllers, and autonomous vehicles."}, {"qnum": 15, "question": "What is one way to ensure ethical considerations in machine learning?", "options": ["Ignoring data privacy concerns", "Prioritising data quantity over quality", "Developing frameworks for ethical decision-making", "Reducing transparency in machine learning"], "correct": 2, "explanation": "Ethical frameworks provide structured guidance for navigating complex moral questions in ML development and deployment. These frameworks typically address fairness (avoiding bias and discrimination), accountability (clear responsibility for decisions), transparency (explainability of how models work), and privacy (protecting sensitive information). Examples include reviewing data for bias, conducting algorithmic impact assessments, implementing human oversight for high-stakes decisions, and ensuring diverse representation in development teams. Without such frameworks, ML systems risk perpetuating or amplifying existing societal biases and causing real harm to individuals and communities."}, {"qnum": 16, "question": "What is the primary difference between supervised and unsupervised learning?", "options": ["Supervised learning requires labeled data, while unsupervised learning does not.", "Supervised learning always produces more accurate results.", "Unsupervised learning is only used for image recognition tasks.", "Supervised learning is faster than unsupervised learning."], "correct": 0, "explanation": "The fundamental distinction is the presence or absence of labeled training data. Supervised learning requires a dataset where each input example is paired with its correct output (label), allowing the model to learn the input-output mapping. Unsupervised learning works with unlabeled data, discovering patterns and structures without being told what the 'correct' answers are. Neither approach is universally 'better'‚Äîthey serve different purposes. Supervised learning is ideal when you have labeled data and want to predict specific outcomes. Unsupervised learning excels at exploration and finding hidden structure when labels are unavailable or when you want to discover unknown patterns."}, {"qnum": 17, "question": "What do ethical frameworks in machine learning aim to provide guidance on?", "options": ["Speeding up machine learning algorithms", "Maximising data storage", "Making ethical decisions related to machine learning", "Reducing transparency"], "correct": 2, "explanation": "Ethical frameworks in ML provide systematic approaches to identifying, analyzing, and addressing moral dilemmas that arise throughout the ML lifecycle. They guide practitioners in questions like: Is this data collected with proper consent? Does our training data contain biases? Could this model discriminate against protected groups? Who is accountable if the model makes a harmful decision? How do we ensure transparency while protecting intellectual property? These frameworks help teams move beyond purely technical considerations to think about broader societal impacts, fairness, justice, and human rights in their work."}, {"qnum": 18, "question": "Which of the following is NOT a focus area of machine learning ethics?", "options": ["Data quality", "Data privacy", "Maximising data collection", "Ensuring fairness and equity"], "correct": 2, "explanation": "ML ethics emphasizes responsible data practices, not indiscriminate data collection. While data quality, privacy, and fairness are core ethical concerns, maximizing data collection for its own sake can actually raise serious ethical issues. Collecting unnecessary personal data increases privacy risks, storage costs, and security vulnerabilities. Ethical practice involves collecting only what's needed (data minimization principle), ensuring proper consent, protecting sensitive information, and considering whether data collection might harm individuals or communities. More data doesn't always mean better models‚Äîit can mean more potential for harm if not handled responsibly."}, {"qnum": 19, "question": "What is the primary objective of clustering in Unsupervised Learning?", "options": ["Predicting a target variable", "Reducing dimensionality", "Grouping similar data points together", "Maximising rewards"], "correct": 2, "explanation": "Clustering algorithms aim to partition data into groups (clusters) where members of each group are more similar to each other than to members of other groups. The algorithm discovers these natural groupings without being told what they should be. For example, clustering customer purchase data might reveal distinct segments: budget-conscious shoppers, premium buyers, and impulse purchasers. Unlike classification (a supervised task where categories are predefined), clustering discovers categories from the data itself. Common algorithms include K-means, hierarchical clustering, and DBSCAN, each with different strengths for different data characteristics."}, {"qnum": 20, "question": "Which type of Machine Learning is most closely associated with reward-based decision making?", "options": ["Supervised learning", "Unsupervised learning", "Reinforcement learning", "Semi-supervised learning"], "correct": 2, "explanation": "Reinforcement learning explicitly models decision-making as a process of maximizing cumulative reward over time. The agent learns through trial and error: it takes actions in an environment, observes the consequences, receives rewards or penalties, and adjusts its strategy (policy) to increase future rewards. This reward signal guides learning without explicit examples of correct behavior. For instance, a robot learning to walk receives positive rewards for forward movement and negative rewards for falling. Over many attempts, it learns which actions lead to better outcomes, gradually developing an effective walking strategy through reward-based feedback."}, {"qnum": 21, "question": "What is the primary goal of data visualisation in EDA?", "options": ["To make data more complex", "To create aesthetically pleasing charts", "To convey insights and patterns in the data", "To replace data cleaning techniques"], "correct": 2, "explanation": "Data visualization in EDA serves as a powerful tool for discovering and communicating patterns, trends, relationships, and anomalies that might not be obvious in raw numbers. The human visual system excels at pattern recognition‚Äîa well-designed visualization can reveal clusters, outliers, distributions, and correlations almost instantly. While aesthetics matter for communication, the primary goal is insight generation. Good visualizations help you spot data quality issues, understand variable distributions, identify relationships between variables, and form hypotheses for further analysis. They complement, rather than replace, statistical analysis and data cleaning."}, {"qnum": 22, "question": "Which EDA technique focuses on analysing one variable at a time?", "options": ["Bivariate exploration", "Multivariate exploration", "Univariate exploration", "Data cleaning"], "correct": 2, "explanation": "Univariate analysis examines each variable in isolation to understand its individual characteristics. This typically involves calculating summary statistics (mean, median, standard deviation, quartiles) and creating visualizations like histograms, box plots, or bar charts. For a continuous variable like age, you might examine its distribution, identify outliers, and check for skewness. For a categorical variable like customer type, you'd look at frequency counts and proportions. Univariate analysis is usually the first step in EDA, providing a foundation before moving to bivariate (two variables) or multivariate (multiple variables) analysis."}, {"qnum": 23, "question": "What is the primary purpose of using box plots in EDA?", "options": ["To visualise the spread of categorical variables", "To compare the central tendencies of multiple variables", "To create scatter plots", "To identify outliers and understand the distribution of a single variable"], "correct": 3, "explanation": "Box plots (also called box-and-whisker plots) provide a compact visual summary of a variable's distribution. They display the median (center line), quartiles (box edges showing the middle 50% of data), and range (whiskers), while clearly marking potential outliers as individual points beyond the whiskers. This makes it easy to quickly assess: Is the data symmetric or skewed? Where is the center? How spread out is it? Are there unusual values? Box plots are particularly useful for comparing distributions across multiple groups side-by-side, making differences in central tendency and spread immediately apparent."}, {"qnum": 24, "question": "What is the primary goal of feature engineering in EDA?", "options": ["To create complex data visualisations", "To select and create relevant variables for analysis", "To identify outliers in the dataset", "To replace missing data"], "correct": 1, "explanation": "Feature engineering during EDA involves creating, selecting, and transforming variables to better capture the information relevant to your analysis or modeling goals. This might include deriving new features from existing ones (like creating 'age_group' from continuous 'age'), combining features (calculating 'price_per_square_foot' from 'price' and 'area'), extracting components (pulling 'day_of_week' from timestamps), or transforming features (log-transforming skewed data). Good feature engineering informed by EDA insights can dramatically improve model performance and make patterns more apparent. It requires both domain knowledge and creative thinking about how to represent information."}, {"qnum": 25, "question": "Which of the following is an essential step in preparing data for EDA?", "options": ["Choosing variables randomly", "Ensuring data is properly formatted and cleaned", "Removing all missing values from the dataset", "Conducting multivariate exploration first"], "correct": 1, "explanation": "Before diving into EDA, you need clean, well-formatted data that your analysis tools can work with. This preparation involves checking data types (ensuring numbers are stored as numeric, not text), standardizing formats (consistent date formats, unified category names), handling obvious errors, and addressing structural issues. However, this doesn't mean eliminating all missing values‚Äîunderstanding patterns in missingness is actually part of EDA. The goal is to get your data into a state where you can reliably explore it, without being derailed by technical issues like incompatible formats or corrupted entries."}, {"qnum": 26, "question": "In EDA, what does \"data anonymisation\" refer to?", "options": ["Removing outliers from the dataset", "Encrypting the data for security purposes", "Making data available to the public", "Protecting individuals' identities in the data"], "correct": 3, "explanation": "Data anonymization involves removing or masking personally identifiable information (PII) so that individuals cannot be identified from the data. Techniques include removing direct identifiers (names, ID numbers, email addresses), generalizing specific values (replacing exact ages with age ranges), suppressing rare combinations of attributes that could identify individuals, and using k-anonymity principles to ensure each record is indistinguishable from at least k-1 others. Proper anonymization is crucial when sharing data for analysis or research, especially with regulations like GDPR requiring privacy protection. However, it's challenging‚Äîseemingly anonymous data can sometimes be re-identified by linking with other datasets."}, {"qnum": 27, "question": "What is a potential drawback of oversimplifying EDA by relying solely on automated tools?", "options": ["It speeds up the EDA process.", "It enhances data privacy.", "It eliminates the need for data visualisation.", "It can miss subtle data patterns or anomalies"], "correct": 3, "explanation": "While automated EDA tools can quickly generate standard statistics and visualizations, they may miss domain-specific patterns, subtle anomalies, or context-dependent issues that an experienced analyst would catch. Automated tools apply generic analyses without understanding your specific business context, data generating process, or analytical goals. They might generate dozens of charts without highlighting which ones matter for your problem. Human insight is crucial for: recognizing when unusual patterns are meaningful versus noise, understanding causality versus correlation, knowing which relationships to investigate deeper, and connecting data patterns to real-world implications. The best approach combines automation's efficiency with human expertise and judgment."}, {"qnum": 28, "question": "In EDA, what is the primary purpose of examining the distribution of a variable?", "options": ["To identify outliers", "To calculate the mean", "To create data visualisations", "To understand the spread and central tendency of the data"], "correct": 3, "explanation": "Understanding a variable's distribution reveals fundamental characteristics: Where does most of the data concentrate (central tendency)? How spread out are the values (dispersion)? Is it symmetric or skewed? Are there multiple peaks (modes)? Understanding distribution shapes informs analytical choices‚Äînormally distributed data suits certain statistical tests, while skewed data might need transformation. Distributions also reveal data quality issues and suggest modeling approaches. While outlier identification and calculating means are aspects of examining distributions, the broader goal is comprehending the overall pattern of how values are distributed across the range."}, {"qnum": 29, "question": "In EDA, what is the purpose of data cleaning?", "options": ["Creating summary statistics", "Making data more complex", "Removing outliers and errors", "Selecting variables for analysis"], "correct": 2, "explanation": "Data cleaning addresses quality issues that would compromise analysis: correcting errors (typos, impossible values), handling missing data appropriately, removing duplicates, standardizing inconsistent entries (different spellings of the same category), and dealing with outliers (determining if they're errors or legitimate extreme values). The goal isn't to make data 'perfect' but to ensure it's fit for analysis. Not all outliers should be removed‚Äîsome represent important rare events. Similarly, handling missing data depends on why it's missing. Good cleaning preserves signal while removing noise, improving the reliability and validity of subsequent analysis."}, {"qnum": 30, "question": "In EDA, what is the primary purpose of conducting hypothesis testing?", "options": ["To identify outliers", "To test specific hypotheses or research questions about the data", "To create data visualisations", "To calculate summary statistics"], "correct": 1, "explanation": "Hypothesis testing provides statistical rigor to claims suggested by EDA. After exploring data and forming hypotheses about relationships or differences (e.g., 'customers who use feature X have higher retention rates'), hypothesis tests quantify the evidence for these claims. Tests help distinguish real patterns from random noise by calculating the probability of observing your results if no real effect exists (p-value). While EDA might suggest that two groups differ, a hypothesis test tells you whether that difference is statistically significant or could easily occur by chance. This moves you from exploratory observations to confirmatory evidence that can support decisions or further research."}, {"qnum": 31, "question": "Which of the following is an example of feature engineering?", "options": ["Splitting the dataset into training and testing sets", "Standardizing numeric features", "Creating a new feature based on existing ones", "Cross-validation"], "correct": 2, "explanation": "Feature engineering specifically refers to creating new variables from existing ones to better capture patterns relevant to your problem. Examples include: combining 'height' and 'weight' to create 'BMI', extracting 'hour_of_day' from a timestamp to capture daily patterns, calculating 'customer_lifetime_value' from purchase history, or creating interaction terms like 'age √ó income'. While standardization and train-test splitting are important preprocessing steps, they don't create new features. Good feature engineering requires domain knowledge‚Äîunderstanding what derived variables might be predictive. It's often more impactful than algorithm choice; the right features can make a simple model outperform a complex one with poor features."}, {"qnum": 32, "question": "What are low variance features?", "options": ["Features with a small mean value", "Features with high variability in their values", "Features with little variation or nearly constant values", "Features with high correlation to the target variable"], "correct": 2, "explanation": "Low variance features have values that barely change across observations‚Äîthey're nearly constant. For example, a 'country' column where 99.9% of entries are 'USA' has very low variance. Such features provide little useful information for modeling because they don't distinguish between different observations. If a feature is almost the same for everyone, it can't help predict differences in outcomes. Models may even overfit to the tiny variations present. Removing low variance features during feature selection reduces dimensionality, improves model efficiency, and eliminates noise without losing predictive information. The threshold for 'low' variance depends on your data and context."}, {"qnum": 33, "question": "Why is feature engineering important in machine learning?", "options": ["It reduces the number of features.", "It simplifies model evaluation.", "It improves model performance.", "It automates data collection."], "correct": 2, "explanation": "Feature engineering is often the single most impactful step in ML workflows. Well-engineered features can dramatically improve model performance by: (1) making patterns more apparent and easier for algorithms to learn, (2) incorporating domain knowledge that the model wouldn't discover from raw data alone, (3) creating representations better suited to the algorithm's assumptions, and (4) reducing the need for more complex models. For instance, creating a 'days_since_last_purchase' feature for churn prediction encodes crucial information that's implicit but hard to extract from raw transaction dates. Often, mediocre algorithms with great features outperform sophisticated algorithms with poor features."}, {"qnum": 34, "question": "Feature engineering can involve:", "options": ["Only numerical features", "Only categorical features", "Both numerical and categorical features", "Neither numerical nor categorical features"], "correct": 2, "explanation": "Feature engineering applies to all data types. For numerical features, you might create polynomial terms, apply mathematical transformations (log, sqrt), bin values into categories, or calculate rolling statistics. For categorical features, you might create aggregated statistics (mean target value per category), combine categories, extract components from text, or create binary indicators. Often the most powerful features combine both types‚Äîfor example, calculating 'average purchase amount per customer segment' combines numerical (purchase amount) and categorical (segment) information. The key is thinking creatively about what derived information might be predictive, regardless of the original data type."}, {"qnum": 35, "question": "LDA is a supervised or unsupervised dimension reduction technique?", "options": ["Supervised", "Unsupervised", "Semi-supervised", "None of the above"], "correct": 0, "explanation": "Linear Discriminant Analysis (LDA) is a supervised technique because it uses class labels to find the best projection of data. Unlike PCA (unsupervised), which maximizes variance regardless of classes, LDA specifically seeks directions that maximize the separation between different classes while minimizing the spread within each class. It projects data onto a lower-dimensional space where classes are as distinguishable as possible. For example, given features describing iris flowers and their species labels, LDA finds the combination of features that best separates the species. This makes LDA valuable not just for dimensionality reduction but also for classification tasks."}, {"qnum": 36, "question": "What is the primary goal of Feature Selection in machine learning?", "options": ["Increasing model complexity", "Reducing the risk of overfitting and improving model performance", "Reducing the training time of models", "Maximizing the number of features in the dataset"], "correct": 1, "explanation": "Feature selection identifies and keeps only the most relevant features while removing irrelevant or redundant ones. This achieves multiple benefits: (1) Reduces overfitting by eliminating noise that models might learn as patterns, (2) Improves model interpretability by focusing on truly important variables, (3) Decreases computational costs, and (4) Sometimes improves accuracy by removing misleading features. While faster training is a benefit, the primary goal is better generalization‚Äîmodels that perform well on new data. Feature selection is especially crucial in high-dimensional datasets where many features may be irrelevant, redundant, or noisy, potentially confusing the model and degrading performance."}, {"qnum": 37, "question": "In LDA, what is the within-class scatter matrix used for?", "options": ["To measure the total variance in the dataset", "To measure the separation between classes", "To measure the spread of data points within each class", "To calculate eigenvalues"], "correct": 2, "explanation": "The within-class scatter matrix quantifies how spread out the data points are around their respective class means‚Äîessentially measuring the variance within each class. LDA's goal is to find projections that minimize this within-class scatter (making each class tight and compact) while maximizing between-class scatter (making different classes far apart). Think of it like organizing similar items: you want items in the same category grouped closely together (low within-class scatter) while keeping different categories well separated (high between-class scatter). The ratio of between-class to within-class scatter determines the quality of the discriminative projection that LDA seeks."}, {"qnum": 38, "question": "In which stage of the machine learning pipeline is it most common to create interaction variables?", "options": ["Data collection", "Data preprocessing", "Model training", "Model evaluation"], "correct": 1, "explanation": "Interaction variables are typically created during data preprocessing/feature engineering, after initial data cleaning but before model training. This stage is where you transform and augment your feature set to better represent patterns in the data. Creating interactions‚Äîlike multiplying 'advertising_spend' √ó 'market_size' to capture their combined effect on sales‚Äîrequires understanding both the data and the domain. These engineered features are then fed into the model during training. While you might discover the need for certain interactions during model evaluation (if simple features don't capture relationships well), the actual creation happens in the preprocessing stage."}, {"qnum": 39, "question": "What is the primary goal of dimension reduction techniques in machine learning?", "options": ["To increase the dimensionality of the data", "To reduce the computational complexity", "To reduce the number of features while preserving relevant information", "To increase overfitting"], "correct": 2, "explanation": "Dimensionality reduction compresses high-dimensional data into fewer dimensions while retaining as much meaningful information as possible. The goal is to eliminate redundancy and noise without losing the signal. For example, if you have 100 correlated measurements of similar aspects, you might capture 95% of the information in just 10 principal components. Benefits include: avoiding the curse of dimensionality (where high dimensions make data sparse and patterns harder to find), improving visualization (projecting to 2-3 dimensions), reducing computational costs, and often improving model performance by removing noisy dimensions. The key is preserving what matters‚Äîvariance (PCA) or class separability (LDA)‚Äîwhile discarding what doesn't."}, {"qnum": 40, "question": "In dimension reduction, what does \"curse of dimensionality\" refer to?", "options": ["A situation where there are too few features to work with", "A situation where there are too many features, making the data difficult to work with", "A situation where dimension reduction is not possible", "A situation where features have high correlations"], "correct": 1, "explanation": "The curse of dimensionality describes various problems that emerge as the number of features grows: (1) Data becomes increasingly sparse‚Äîpoints that seem close in low dimensions are actually far apart when considering all dimensions; (2) Distance measures become less meaningful‚Äîin high dimensions, the distance between nearest and farthest points becomes similar; (3) Sample size requirements explode‚Äîyou need exponentially more data to maintain the same density; (4) Computational costs increase dramatically. Imagine trying to sample 10 points across a line (1D), then a square (2D), then a cube (3D)‚Äîthe volume to cover grows exponentially, but your sample size stays constant, making the data progressively sparser."}, {"qnum": 41, "question": "Which statistical technique can help identify meaningful interaction variables?", "options": ["Principal Component Analysis (PCA)", "t-SNE (t-distributed Stochastic Neighbor Embedding)", "Correlation analysis", "Chi-squared test"], "correct": 2, "explanation": "Correlation analysis helps identify potential interactions by revealing relationships between variables and the target. If you calculate correlations between product terms (like 'feature_A √ó feature_B') and your target variable, strong correlations suggest that interaction might be meaningful. You can also examine correlations among features themselves‚Äîhighly correlated features might interact in interesting ways. While techniques like PCA reduce dimensions and t-SNE visualizes relationships, they don't directly identify specific interaction terms. Domain knowledge remains crucial‚Äîcorrelation suggests where to look, but understanding why variables might interact (e.g., 'temperature √ó humidity' affecting comfort) requires contextual insight."}, {"qnum": 42, "question": "When do you typically use dummy variables in machine learning?", "options": ["When dealing with continuous data", "When dealing with missing values", "When dealing with categorical data", "When building neural networks"], "correct": 2, "explanation": "Dummy variables (one-hot encoding) convert categorical variables into a format that machine learning algorithms can process. Most ML algorithms require numerical input, so categories like 'Red', 'Blue', 'Green' must be transformed. One-hot encoding creates binary (0/1) columns for each category: a 'Red' item gets [1, 0, 0], 'Blue' gets [0, 1, 0], etc. This avoids implying ordinal relationships that wouldn't exist if you simply numbered categories (1='Red', 2='Blue' would incorrectly suggest 'Blue' > 'Red'). Note: you typically use k-1 dummy variables for k categories to avoid perfect collinearity (the dummy variable trap)."}, {"qnum": 43, "question": "When might creating interaction variables be beneficial?", "options": ["When dealing with missing data", "When simplifying the model", "When there are non-linear relationships between features", "When the dataset is small"], "correct": 2, "explanation": "Interaction variables capture situations where the effect of one feature depends on the value of another‚Äîa form of non-linearity. For example, the impact of 'advertising spend' on sales might depend on 'brand awareness': advertising is highly effective when brand awareness is high but less effective when it's low. The product term 'advertising √ó brand_awareness' captures this interaction. Linear models with interaction terms can model certain non-linear relationships. Interactions are especially valuable when domain knowledge suggests features work together synergistically or antagonistically. However, they increase model complexity and require sufficient data to estimate reliably, so they're not always beneficial for small datasets."}, {"qnum": 44, "question": "What are interaction variables in feature engineering?", "options": ["Variables that interact with the target variable", "Variables created by multiplying or combining existing features", "Variables that are highly correlated with each other", "Variables used for dimensionality reduction"], "correct": 1, "explanation": "Interaction variables are new features created by combining existing ones, typically through multiplication but sometimes through other operations. The classic example is X‚ÇÅ √ó X‚ÇÇ, which allows the effect of X‚ÇÅ to vary with X‚ÇÇ's value. For instance, combining 'employee_experience' √ó 'training_hours' might better predict productivity than either alone because training's impact depends on experience level. Interactions can also involve addition, division, or more complex combinations. They're particularly important when the relationship between predictors and outcome is multiplicative rather than purely additive. While all features ultimately 'interact' with the target, interaction variables specifically model how features influence each other's effects."}, {"qnum": 45, "question": "Which of the following can be an outcome of applying PCA or LDA to a dataset?", "options": ["Increased feature dimensionality", "Improved model interpretability", "Reduced data dimensionality while preserving relevant information", "Reduced computation time"], "correct": 2, "explanation": "Both PCA and LDA primarily achieve dimensionality reduction while preserving information‚ÄîPCA retains variance, LDA retains class separability. By projecting high-dimensional data into fewer dimensions, they compress information into a more manageable form. While reduced computation time is often a beneficial side effect, it's not the primary outcome. Regarding interpretability: this can go either way. The new dimensions (principal components or discriminant functions) are combinations of original features, which can make them harder to interpret ('PC1 = 0.3√ófeature_A + 0.7√ófeature_B - 0.4√ófeature_C' isn't intuitive). However, reducing from 100 features to 5 can make overall patterns more interpretable through visualization."}, {"qnum": 46, "question": "In target encoding, how are categorical variables transformed into numerical values?", "options": ["By creating new columns for each category", "By replacing each category with its frequency", "By using the target variable's mean for each category", "By performing a logarithmic transformation"], "correct": 2, "explanation": "Target encoding (also called mean encoding) replaces each category with a statistic of the target variable for that category, typically the mean. For example, if encoding 'city' to predict income, each city would be replaced by the average income of people from that city. This creates a natural ordering based on the target and can be more efficient than one-hot encoding for high-cardinality categoricals. However, it risks overfitting (especially with rare categories) and data leakage if not done carefully with cross-validation. Regularization techniques like smoothing (blending category means with the global mean) help address these issues."}, {"qnum": 47, "question": "Which feature engineering technique is useful for handling high cardinality categorical variables?", "options": ["Feature scaling", "One-hot encoding", "Target encoding", "PCA"], "correct": 2, "explanation": "High cardinality categoricals (like customer IDs, zip codes, product SKUs with thousands of unique values) pose problems for one-hot encoding‚Äîcreating thousands of sparse binary columns is computationally expensive and leads to overfitting. Target encoding solves this by collapsing all categories into a single numerical column, where each category's value represents its relationship with the target. For example, encoding 10,000 zip codes creates just one column (mean target value per zip code) instead of 10,000 binary columns. Other techniques for high cardinality include: frequency encoding (count of occurrences), embedding layers (in neural networks), or grouping rare categories together before encoding."}, {"qnum": 48, "question": "What is feature engineering in machine learning?", "options": ["Creating new features from existing data", "Fine-tuning model hyperparameters", "Selecting the right machine learning algorithm", "Writing code for data preprocessing"], "correct": 0, "explanation": "Feature engineering is the process of using domain knowledge and creativity to create new variables (features) from raw data that better represent the underlying problem to the machine learning model. It transforms raw data into a format that makes patterns more apparent and easier for algorithms to learn. Examples include: extracting year/month/day from dates, calculating ratios between variables, creating interaction terms, binning continuous variables, aggregating transaction-level data to customer level, or extracting text features from documents. It's often considered an art because it requires both technical skill and deep understanding of the domain. Good feature engineering can turn a mediocre model into an excellent one."}, {"qnum": 49, "question": "Why might you create interaction variables?", "options": ["To make the dataset smaller", "To simplify the model", "To capture complex relationships between features", "To remove outliers from the data"], "correct": 2, "explanation": "Interaction variables capture complex, non-additive relationships where the effect of one variable depends on another. Consider predicting plant growth: the effect of 'water' isn't independent of 'sunlight'‚Äîplants need both, and optimal water depends on available sunlight. The interaction term 'water √ó sunlight' models this synergy. Without interactions, models assume effects are purely additive (each feature contributes independently), which is often unrealistic. Interactions allow linear models to represent certain non-linear relationships and can significantly improve predictive power when such dependencies exist. However, they add complexity and parameters, so they should be created based on domain knowledge or exploratory analysis suggesting their necessity."}, {"qnum": 50, "question": "What is the primary objective of Linear Discriminant Analysis (LDA)?", "options": ["To maximize class separability by finding the best linear combination of features", "To reduce the dimensionality of data", "To perform unsupervised clustering", "To maximize the variance within each class"], "correct": 0, "explanation": "LDA's core objective is finding the linear combination of features that best separates different classes. It projects data onto a new axis (or axes) where class separation is maximized‚Äîspecifically, maximizing the ratio of between-class variance to within-class variance. Imagine two overlapping clusters: LDA finds the direction that, when you project the points onto it, creates the greatest separation between cluster centers relative to the spread within each cluster. While dimensionality reduction is a common outcome (the new space has at most k-1 dimensions for k classes), it's a means to the end of maximizing discriminability. This makes LDA useful both as a preprocessing step for classification and as a classifier itself."}, {"qnum": 51, "question": "How is variance preserved in PCA?", "options": ["By selecting the features with the highest variance", "By projecting the data onto a lower-dimensional space while maximizing the total variance", "By removing all features with low variance", "By increasing the dimensionality of the data"], "correct": 1, "explanation": "PCA preserves variance by finding orthogonal directions (principal components) that capture the maximum possible variance in the data. The first principal component points in the direction of greatest variance. The second component is orthogonal to the first and captures the most remaining variance, and so on. When you project data onto these components and keep only the first k components, you retain more variance than any other k-dimensional projection would. For example, if the first 3 components explain 95% of variance, you've compressed your data to 3 dimensions while retaining 95% of the information (in terms of variance). This is fundamentally different from just picking high-variance features‚Äîit creates new dimensions that are optimal combinations of all original features."}, {"qnum": 52, "question": "What is the main goal of Principal Component Analysis (PCA)?", "options": ["To maximize class separability", "To identify outliers in the data", "To reduce the dimensionality of data while preserving variance", "To perform feature selection"], "correct": 2, "explanation": "PCA aims to compress high-dimensional data into fewer dimensions while retaining as much variance (information) as possible. It transforms potentially correlated variables into uncorrelated principal components, ordered by how much variance they explain. The key insight is that many datasets have redundancy‚Äîfeatures that are correlated don't add independent information. PCA exploits this by finding the most informative directions in feature space. For example, if you have 50 correlated measurements of related quantities, PCA might reveal that just 5 principal components capture 90% of the variance, allowing effective compression. Unlike feature selection (which chooses a subset of original features), PCA creates entirely new features as combinations of originals."}, {"qnum": 53, "question": "What is target encoding used for in machine learning?", "options": ["Encoding categorical features", "Encoding the target variable", "Encoding numerical features", "Encoding text data"], "correct": 0, "explanation": "Target encoding is specifically a technique for encoding categorical features (not the target itself) by using information from the target variable. It replaces each category with a statistic derived from the target variable‚Äîtypically the mean target value for that category. For example, when predicting whether customers will churn, you might encode 'subscription_type' by replacing each type with the churn rate for that type. This differs from one-hot encoding (which ignores the target) and from encoding the target itself (which you typically don't need to do unless it's multiclass). Target encoding is powerful but requires care to avoid overfitting and data leakage."}, {"qnum": 54, "question": "Which of these techniques is primarily used for dimensionality reduction?", "options": ["Target encoding", "Feature scaling", "Principal Component Analysis (PCA)", "Dummy variables"], "correct": 2, "explanation": "PCA is explicitly and primarily a dimensionality reduction technique‚Äîits entire purpose is to reduce the number of dimensions while preserving information. Target encoding transforms categorical variables but doesn't necessarily reduce dimensionality (though it can make high-cardinality categoricals more manageable). Feature scaling (standardization, normalization) doesn't change the number of features, just their scales. Dummy variables actually increase dimensionality by expanding each categorical variable into multiple binary columns. Other common dimensionality reduction techniques include LDA, t-SNE, UMAP, and autoencoders, each with different strengths and objectives."}, {"qnum": 55, "question": "Which of the following best describes Feature Selection?", "options": ["Adding new features to improve model performance", "Reducing the number of irrelevant or redundant features for model building", "Modifying the existing features to make them more important", "None of the above"], "correct": 1, "explanation": "Feature selection is about identifying and keeping only the most relevant features from your existing feature set, while discarding irrelevant or redundant ones. Unlike feature engineering (which creates new features) or feature extraction like PCA (which transforms features into new representations), feature selection works with the original features and chooses a subset. Methods include: filter methods (scoring features independently using statistical tests), wrapper methods (evaluating feature subsets based on model performance), and embedded methods (feature selection built into the model training, like Lasso regularization). The goal is a more efficient, interpretable model that generalizes better by eliminating noise and reducing overfitting risk."}, {"qnum": 56, "question": "Which dimension reduction technique focuses on maximizing class separability?", "options": [], "correct": 1, "explanation": "LDA (Linear Discriminant Analysis) is specifically designed to maximize class separability. Unlike PCA (which maximizes variance without considering classes), LDA is supervised‚Äîit uses class labels to find projections that best separate different classes. It maximizes the ratio of between-class variance to within-class variance, essentially finding directions where classes are far apart from each other but tight within themselves. This makes LDA ideal when your goal involves classification. PCA might find directions with high variance that don't actually separate classes well (like variations within a class). Factor Analysis focuses on identifying latent variables. t-SNE is primarily for visualization of local structure, not for maximizing class separability."}, {"qnum": 57, "question": "When should you consider using dimension reduction techniques in machine learning?", "options": ["When you have too few features", "When you want to increase the dimensionality of the data", "When you have high-dimensional data or want to remove irrelevant features", "When you want to perform clustering"], "correct": 2, "explanation": "Dimensionality reduction is valuable when: (1) You have high-dimensional data where many features are correlated or redundant, (2) You're suffering from the curse of dimensionality (sparse data, computational costs), (3) You want to visualize high-dimensional data in 2D/3D, (4) You need to reduce overfitting risk by eliminating noisy features, (5) Computational resources or training time are limiting factors. It's not needed when you have few features or when each feature provides unique, valuable information. While clustering can work with reduced dimensions, you don't reduce dimensions specifically to cluster‚Äîrather, you might reduce first to make clustering more effective in high-dimensional spaces."}, {"qnum": 58, "question": "What is the main objective of feature engineering?", "options": ["To replace missing values", "To create as many features as possible", "To prepare the data for machine learning algorithms", "To reduce model complexity"], "correct": 2, "explanation": "Feature engineering's main objective is preparing data in a form that machine learning algorithms can effectively learn from. This means creating features that are: (1) informative‚Äîcapturing patterns relevant to the problem, (2) properly formatted‚Äînumerical when needed, appropriately scaled, (3) aligned with algorithm assumptions‚Äîfor example, some algorithms work better with normalized features, and (4) representative of domain knowledge‚Äîencoding human insight about what matters. It's not about maximizing feature count (which can cause overfitting) or just handling missing values (that's data cleaning). Good feature engineering makes the underlying patterns more explicit and accessible to the learning algorithm, dramatically improving performance with the same model."}, {"qnum": 59, "question": "Which of the following is a common problem associated with high-dimensional data?", "options": ["Underfitting", "Overfitting", "Bias", "Variance"], "correct": 1, "explanation": "High-dimensional data is particularly prone to overfitting. With many features relative to the number of observations, models have enormous flexibility to fit the training data‚Äîincluding fitting noise as if it were signal. Imagine having 100 features but only 50 training examples: the model can essentially 'memorize' the training set by finding complex combinations of features that fit perfectly but don't generalize. This is exacerbated by the curse of dimensionality, where data becomes sparse and distance measures become less meaningful. While variance (which causes overfitting) is technically the problem, 'overfitting' more directly describes the practical issue. Regularization, dimensionality reduction, and more training data help address this."}, {"qnum": 60, "question": "Which of the following is true regarding interaction variables?", "options": ["They are never useful in machine learning models", "They always lead to multicollinearity issues", "They can capture complex relationships between features", "They are only applicable to deep learning models"], "correct": 2, "explanation": "Interaction variables can indeed capture complex relationships where the effect of one feature depends on another‚Äîmaking them very useful across all types of ML models, not just deep learning. While they can introduce multicollinearity (interaction terms are often correlated with their component features), this doesn't always cause problems, especially with regularization or in tree-based models. The key is thoughtful creation: interactions based on domain knowledge or exploratory analysis tend to be valuable, while randomly generating all possible interactions creates noise and overfitting risk. Interactions allow even simple linear models to represent certain non-linearities, often improving performance substantially when the relationships truly exist in the data."}, {"qnum": 61, "question": "What is the chi-squared test used for in the context of feature selection?", "options": ["To measure the correlation between two continuous features", "To test the independence of categorical features", "To compute the mean squared error of a regression model", "To rank features based on their mean values"], "correct": 1, "explanation": "The chi-squared test in feature selection assesses the independence between categorical features and a categorical target. It measures whether the distribution of the target variable differs across categories of the feature. For example, if selecting features to predict customer churn (yes/no), a chi-squared test on 'subscription_type' would test whether churn rates differ significantly across subscription types. A high chi-squared statistic (low p-value) suggests the feature is informative‚Äîthe target depends on it. Features with low chi-squared scores (high p-values) can be removed as they show little relationship with the target. For continuous features or targets, you'd use different tests like Pearson correlation or ANOVA."}, {"qnum": 62, "question": "What is the purpose of dummy variables?", "options": ["To eliminate outliers in the data", "To handle missing values", "To represent categorical data as binary values", "To scale numerical features"], "correct": 2, "explanation": "Dummy variables (one-hot encoding) transform categorical variables into a format machine learning algorithms can process by creating binary (0/1) indicator columns. For a categorical variable with k categories, you create k binary columns (or k-1 to avoid multicollinearity). Each observation gets a 1 in the column corresponding to its category and 0s elsewhere. For example, 'Color' with values {Red, Blue, Green} becomes three columns: Color_Red, Color_Blue, Color_Green. This representation avoids imposing false ordinal relationships (numbering categories 1,2,3 would incorrectly suggest Green > Blue > Red) while making the information usable by models expecting numerical input."}, {"qnum": 63, "question": "Why are low variance features usually removed during feature selection?", "options": ["They provide valuable information to the model", "They are computationally expensive to use", "They do not contribute much information to the model's predictions", "They are always noisy and unreliable"], "correct": 2, "explanation": "Low variance features have values that barely change across observations‚Äîthey're nearly constant. Such features provide little useful information because they don't help distinguish between different observations or outcomes. If everyone has essentially the same value, that value can't explain differences in the target variable. For example, a 'country' column that's 'USA' for 99.99% of records won't help predict anything‚Äîit lacks discriminative power. Removing these features reduces dimensionality without losing information, improves model training efficiency, and can enhance performance by reducing noise. However, 'low' is context-dependent: in a dataset about US customers, state might have low variance; in a global dataset, it's highly variable."}, {"qnum": 64, "question": "In PCA, what are the principal components?", "options": ["The original features in the dataset", "The eigenvectors of the covariance matrix of the data", "The first and last data points", "The classes in a classification problem"], "correct": 1, "explanation": "Principal components are the eigenvectors of the data's covariance (or correlation) matrix, and they represent the directions of maximum variance in the data. Mathematically, these eigenvectors define new coordinate axes for your data space. Each principal component is a linear combination of the original features: PC1 might be 0.5√ófeature1 + 0.3√ófeature2 - 0.8√ófeature3, etc. The associated eigenvalues indicate how much variance each component captures. Components are ordered by their eigenvalues‚ÄîPC1 captures the most variance, PC2 the second most (subject to being orthogonal to PC1), and so on. When you 'do PCA,' you're essentially rotating and scaling your coordinate system to align with these directions of maximum variance, then keeping only the top components."}, {"qnum": 65, "question": "What does the term \"data transformation\" refer to in EDA?", "options": ["Changing the dataset's file format", "Converting categorical variables to numerical values", "Cleaning the data", "Applying mathematical operations to the data to make it suitable for analysis"], "correct": 3, "explanation": "EDA is about surfacing structure, anomalies, and hypotheses via summary statistics and visualisation. Its purpose is insight and bias detection, not prediction per se."}, {"qnum": 66, "question": "Why is fostering transparency important in machine learning?", "options": ["To confuse users", "To discourage ethical considerations", "To promote trust and understanding in machine learning systems", "To increase data complexity"], "correct": 2, "explanation": "The correct option (‚ÄúTo promote trust and understanding in machine learning systems‚Äù) matches the standard definition used in statistics/ML for this topic. The alternatives either conflate concepts or describe secondary objectives."}, {"qnum": 67, "question": "What is the primary role of an algorithm in Machine Learning?", "options": ["To collect data", "To preprocess data", "To make predictions or decisions", "To create features"], "correct": 2, "explanation": "The correct option (‚ÄúTo make predictions or decisions‚Äù) matches the standard definition used in statistics/ML for this topic. The alternatives either conflate concepts or describe secondary objectives."}, {"qnum": 68, "question": "Why is transparency important in machine learning?", "options": ["To hide the machine learning process from users", "To make machine learning models less interpretable", "To foster trust and understanding in machine learning systems", "To speed up machine learning algorithms"], "correct": 2, "explanation": "The correct option (‚ÄúTo foster trust and understanding in machine learning systems‚Äù) matches the standard definition used in statistics/ML for this topic. The alternatives either conflate concepts or describe secondary objectives."}, {"qnum": 69, "question": "Which of the following is NOT a key concept underpinning Machine Learning?", "options": ["Algorithms", "Data", "Logic", "Model"], "correct": 2, "explanation": "The correct option (‚ÄúLogic‚Äù) matches the standard definition used in statistics/ML for this topic. The alternatives either conflate concepts or describe secondary objectives."}, {"qnum": 70, "question": "Sampling refers to in data collection:", "options": ["Collecting all available data", "Collecting a subset of data for analysis", "Collecting only missing data points", "Collecting irrelevant data"], "correct": 1, "explanation": "The correct option (‚ÄúCollecting a subset of data for analysis‚Äù) matches the standard definition used in statistics/ML for this topic. The alternatives either conflate concepts or describe secondary objectives."}, {"qnum": 71, "question": "In data imputation, pairwise deletion refers to:", "options": ["Replacing missing values with the mean", "Filling missing values based on similar records", "Discarding entire rows with any missing values in any column", "Filling missing values using regression"], "correct": 2, "explanation": "In pairwise deletion you use all cases that have data for the variables involved in a specific analysis, so different correlations may use different subsets. It preserves data but can yield inconsistent sample sizes."}, {"qnum": 72, "question": "Primary goal when addressing sampling bias issue:", "options": ["Increasing data complexity", "Ensuring the sample is representative of the population", "Ignoring differences between samples", "Reducing the sample size"], "correct": 1, "explanation": "The correct option (‚ÄúEnsuring the sample is representative of the population‚Äù) matches the standard definition used in statistics/ML for this topic. The alternatives either conflate concepts or describe secondary objectives."}, {"qnum": 73, "question": "How can researchers reduce selection bias in sampling?", "options": ["By using convenience sampling", "By ensuring randomness in sample selection", "By choosing data from a single group", "By ignoring representativeness"], "correct": 1, "explanation": "Selection bias arises when the sampled data is not representative‚Äîe.g., only surveying users who completed signup. Random, stratified, or weighted sampling methods help reduce it."}, {"qnum": 74, "question": "Selection bias refers to in the context of sampling:", "options": ["Random errors in data collection", "Bias introduced by selecting non-random or non-representative samples", "Differences due to large sample sizes", "Variation caused by data scaling"], "correct": 1, "explanation": "Selection bias arises when the sampled data is not representative‚Äîe.g., only surveying users who completed signup. Random, stratified, or weighted sampling methods help reduce it."}, {"qnum": 75, "question": "Primary advantage of using advanced imputation methods like multiple imputation:", "options": ["They eliminate missing values completely", "They provide a range of imputed datasets, capturing uncertainty in imputation", "They are faster than mean imputation", "They reduce data dimensionality"], "correct": 1, "explanation": "Multiple imputation replaces each missing value with several plausible draws, generating multiple datasets. Models are trained on each and results are pooled, capturing uncertainty from the imputation process."}, {"qnum": 76, "question": "How does stratified sampling contribute to data representativeness?", "options": ["It randomly selects samples without considering subgroups", "It accounts for the proportion of different groups in the population", "It ignores minority classes", "It reduces sample diversity"], "correct": 1, "explanation": "Stratified sampling divides the population into homogeneous strata and samples proportionally from each, ensuring subgroup representation and a more representative sample than purely random sampling when groups are imbalanced."}, {"qnum": 77, "question": "Data representativeness refers to:", "options": ["The extent to which the data reflects the real-world scenario", "The size of the dataset", "The accuracy of data labels", "The speed of data collection"], "correct": 0, "explanation": "The correct option (‚ÄúThe extent to which the data reflects the real-world scenario‚Äù) matches the standard definition used in statistics/ML for this topic. The alternatives either conflate concepts or describe secondary objectives."}, {"qnum": 78, "question": "When selecting data sources for machine learning, what should be the primary consideration?", "options": ["Availability of data regardless of relevance", "Choosing data sources that align with the problem domain and objectives", "Selecting data with the largest number of variables", "Collecting data with maximum volume"], "correct": 1, "explanation": "The correct option (‚ÄúChoosing data sources that align with the problem domain and objectives‚Äù) matches the standard definition used in statistics/ML for this topic. The alternatives either conflate concepts or describe secondary objectives."}, {"qnum": 79, "question": "In data quality considerations, accuracy implies:", "options": ["Data is correct and precise", "Data is duplicated", "Data is random and inconsistent", "Data is incomplete"], "correct": 0, "explanation": "Accuracy means measurements are close to the true value (correct and precise). In data quality it reflects correctness of recorded values, not just consistency or completeness."}, {"qnum": 80, "question": "CRISP-DM is a widely used methodology in:", "options": ["Image processing", "Data mining and machine learning", "Text summarization", "Database normalization"], "correct": 1, "explanation": "The correct option (‚ÄúData mining and machine learning‚Äù) matches the standard definition used in statistics/ML for this topic. The alternatives either conflate concepts or describe secondary objectives."}, {"qnum": 81, "question": "How can you address noisy data?", "options": ["Using statistical methods to filter out noise", "Ignoring the outliers", "Adding more noise for consistency", "Reducing data volume"], "correct": 0, "explanation": "The correct option (‚ÄúUsing statistical methods to filter out noise‚Äù) matches the standard definition used in statistics/ML for this topic. The alternatives either conflate concepts or describe secondary objectives."}, {"qnum": 82, "question": "Which technique is appropriate when the missing data are not missing at random (MNAR)?", "options": ["Listwise deletion", "Mean imputation", "Advanced imputation methods such as multiple or regression imputation", "Removing all variables with missing values"], "correct": 2, "explanation": "The correct option (‚ÄúAdvanced imputation methods such as multiple or regression imputation‚Äù) matches the standard definition used in statistics/ML for this topic. The alternatives either conflate concepts or describe secondary objectives."}, {"qnum": 83, "question": "Validity in data quality is concerned with:", "options": ["The accuracy of data representation", "The volume of data collected", "The number of missing values", "The computational cost of processing data"], "correct": 0, "explanation": "The correct option (‚ÄúThe accuracy of data representation‚Äù) matches the standard definition used in statistics/ML for this topic. The alternatives either conflate concepts or describe secondary objectives."}, {"qnum": 84, "question": "Sampling bias occurs when:", "options": ["The sample is selected randomly from the population", "The sample is not representative of the population", "All individuals have equal probability of selection", "The sampling process is unbiased"], "correct": 1, "explanation": "The correct option (‚ÄúThe sample is not representative of the population‚Äù) matches the standard definition used in statistics/ML for this topic. The alternatives either conflate concepts or describe secondary objectives."}, {"qnum": 85, "question": "When conducting multivariate exploration in EDA, what type of visualisations are commonly used to visualise relationships between three variables?", "options": ["Scatter plots", "Bar charts", "Heatmaps", "Histograms"], "correct": 2, "explanation": "EDA is about surfacing structure, anomalies, and hypotheses via summary statistics and visualisation. Its purpose is insight and bias detection, not prediction per se."}, {"qnum": 86, "question": "When performing EDA, what is the significance of identifying outliers?", "options": ["Outliers can indicate errors or unique patterns in the data", "Outliers help to increase data complexity", "Outliers should always be removed from the dataset", "Outliers have no impact on data analysis"], "correct": 0, "explanation": "Outliers can be data errors or rare but important cases. Identifying them helps decide whether to clean errors, transform variables, or use robust models. Removing blindly can delete signal; understanding their source is key."}, {"qnum": 87, "question": "Which of the following is an ethical consideration when using data visualisation in EDA?", "options": ["Using colourful visuals for better presentation", "Ensuring visualisations are easy to interpret", "Avoiding distortion of data to mislead viewers", "Making visualisations as complex as possible"], "correct": 2, "explanation": "EDA is about surfacing structure, anomalies, and hypotheses via summary statistics and visualisation. Its purpose is insight and bias detection, not prediction per se."}, {"qnum": 88, "question": "Which of the following is NOT a valid approach to handle imbalanced datasets during EDA?", "options": ["Oversampling the minority class", "Undersampling the majority class", "Ignoring the class imbalance", "Using synthetic data generation techniques"], "correct": 2, "explanation": "For imbalanced datasets you typically change the training distribution (over/under‚Äësampling) or synthesize new minority samples (e.g., SMOTE). Simply ignoring imbalance leaves the model biased toward the majority class and usually degrades recall on the minority."}, {"qnum": 89, "question": "What should you do if you discover potential biases in your dataset during EDA?", "options": ["Ignore them and continue the analysis", "Report the biases and their implications", "Remove any biased variables", "Conduct additional data cleaning"], "correct": 1, "explanation": "Explanation unavailable by default. Use the 'Set correct answer' control to choose the right option and add a note. Pairwise deletion example: analyses use all cases where the specific pair of variables is present, instead of discarding entire rows."}, {"qnum": 90, "question": "Why is it essential to uphold ethical standards during EDA?", "options": ["To get accurate results", "To protect individuals' privacy and rights", "To speed up the analysis process", "To simplify data visualisation"], "correct": 1, "explanation": "Explanation unavailable by default. Use the 'Set correct answer' control to choose the right option and add a note. Pairwise deletion example: analyses use all cases where the specific pair of variables is present, instead of discarding entire rows."}, {"qnum": 91, "question": "What is a common goal of bivariate exploration in EDA?", "options": ["Understanding the distribution of a single variable", "Analysing relationships between two variables", "Removing outliers from the dataset", "Creating summary statistics"], "correct": 1, "explanation": "Explanation unavailable by default. Use the 'Set correct answer' control to choose the right option and add a note. Pairwise deletion example: analyses use all cases where the specific pair of variables is present, instead of discarding entire rows."}, {"qnum": 92, "question": "Which step of EDA involves summarising the data using measures like mean, median, and standard deviation?", "options": ["Data visualisation", "Univariate exploration", "Data cleaning", "Bivariate exploration"], "correct": 1, "explanation": "Explanation unavailable by default. Use the 'Set correct answer' control to choose the right option and add a note. Pairwise deletion example: analyses use all cases where the specific pair of variables is present, instead of discarding entire rows."}, {"qnum": 93, "question": "What should you do if you encounter missing data during EDA?", "options": ["Replace missing values with zeros", "Exclude rows with missing data from the analysis", "Impute missing values using appropriate techniques", "Ignore missing data and proceed with analysis"], "correct": 2, "explanation": "Explanation unavailable by default. Use the 'Set correct answer' control to choose the right option and add a note. Pairwise deletion example: analyses use all cases where the specific pair of variables is present, instead of discarding entire rows."}, {"qnum": 94, "question": "What should be the primary focus when selecting variables for EDA?", "options": ["Choosing variables that are easy to analyse", "Selecting variables that are highly correlated", "Choosing variables that are relevant to the analysis goals", "Selecting variables randomly"], "correct": 2, "explanation": "Explanation unavailable by default. Use the 'Set correct answer' control to choose the right option and add a note. Pairwise deletion example: analyses use all cases where the specific pair of variables is present, instead of discarding entire rows."}, {"qnum": 95, "question": "When conducting EDA, what is the primary goal of using a scatter plot?", "options": ["To compare the central tendencies of multiple variables", "To visualise the distribution of categorical variables", "To identify outliers and errors in the data", "To explore the relationship between two continuous variables"], "correct": 3, "explanation": "Explanation unavailable by default. Use the 'Set correct answer' control to choose the right option and add a note. Pairwise deletion example: analyses use all cases where the specific pair of variables is present, instead of discarding entire rows."}, {"qnum": 96, "question": "Which of the following is NOT a part of ethical data handling in EDA?", "options": ["Anonymising personal information", "Sharing data without consent", "Obtaining informed consent", "Securing data storage"], "correct": 1, "explanation": "Explanation unavailable by default. Use the 'Set correct answer' control to choose the right option and add a note. Pairwise deletion example: analyses use all cases where the specific pair of variables is present, instead of discarding entire rows."}, {"qnum": 97, "question": "What is the primary purpose of a correlation matrix in EDA?", "options": ["To visualise data distributions", "To measure the strength and direction of relationships between variables", "To identify outliers in the dataset", "To create scatter plots"], "correct": 1, "explanation": "Explanation unavailable by default. Use the 'Set correct answer' control to choose the right option and add a note. Pairwise deletion example: analyses use all cases where the specific pair of variables is present, instead of discarding entire rows."}, {"qnum": 98, "question": "Which of the following is an ethical principle related to EDA?", "options": ["Transparency in data analysis methods", "Maximising data complexity", "Minimising data sharing", "Ignoring potential biases in the data"], "correct": 0, "explanation": "EDA is about surfacing structure, anomalies, and hypotheses via summary statistics and visualisation. Its purpose is insight and bias detection, not prediction per se."}, {"qnum": 99, "question": "What is the primary purpose of Exploratory Data Analysis (EDA)?", "options": ["To make predictions", "To understand data patterns and potential biases", "To create data visualisations", "To perform hypothesis testing"], "correct": 1, "explanation": "EDA is about surfacing structure, anomalies, and hypotheses via summary statistics and visualisation. Its purpose is insight and bias detection, not prediction per se."}];
const KEY_STORAGE = "mcq_answers_v2";
const EXPL_STORAGE = "mcq_explanations_v2";

function loadState() { try { return JSON.parse(localStorage.getItem(KEY_STORAGE) || "{}"); } catch(e){ return {}; } }
function saveState(state) { localStorage.setItem(KEY_STORAGE, JSON.stringify(state)); }
function loadExpl(){ try{return JSON.parse(localStorage.getItem(EXPL_STORAGE)||"{}");}catch(e){return {};} }
function saveExpl(e){ localStorage.setItem(EXPL_STORAGE, JSON.stringify(e)); }

const state = loadState();
const expl = loadExpl();

function render(){
  const root = document.getElementById('quiz');
  root.innerHTML = '';
  DATA.forEach(q => {
    const card = document.createElement('div');
    card.className = 'qcard';
    card.id = 'q'+q.qnum;

    const title = document.createElement('div');
    title.className = 'qtitle';
    title.textContent = q.qnum + '. ' + q.question;
    card.appendChild(title);

    const choices = document.createElement('div');
    choices.className = 'choices';
    q.options.forEach((opt, i) => {
      const lab = document.createElement('label');
      lab.role = "button";
      lab.tabIndex = 0;
      const inp = document.createElement('input');
      inp.type='radio'; inp.name='q'+q.qnum; inp.value=i;
      inp.checked = (state[q.qnum] !== undefined && Number(state[q.qnum]) === i);
      inp.addEventListener('change', () => onSelect(q, i, lab));
      const span = document.createElement('span');
      span.textContent = String.fromCharCode(97+i) + ') ' + opt;
      lab.appendChild(inp); lab.appendChild(span);
      if (state[q.qnum] !== undefined) {
        const correctAns = (expl[q.qnum]?.correct ?? q.correct);
        if (Number(state[q.qnum]) === i && correctAns !== undefined && correctAns !== i) lab.classList.add('choice-bad');
        if (correctAns !== undefined && correctAns === i) lab.classList.add('choice-ok');
      }
      choices.appendChild(lab);
    });
    card.appendChild(choices);

    const res = document.createElement('div');
    res.className = 'result';
    card.appendChild(res);

    const ex = document.createElement('div');
    ex.className = 'explain';
    const exInner = document.createElement('div');
    exInner.innerHTML = '<strong>üìñ Explanation:</strong> ' + (expl[q.qnum]?.text || q.explanation || 'Explanation not provided.');
    ex.appendChild(exInner);
    card.appendChild(ex);

    const edit = document.createElement('div');
    edit.className = 'editbox tools';
    const small = document.createElement('div'); small.className='small'; small.textContent='Adjust answer/explanation (saved locally on this iPad)';
    const sel = document.createElement('select'); sel.setAttribute('aria-label','Set correct answer');
    const opt0 = document.createElement('option'); opt0.value=''; opt0.textContent='-- Set correct option --'; sel.appendChild(opt0);
    for (let i=0;i<q.options.length;i++) { const o=document.createElement('option'); o.value=String(i); o.textContent=String.fromCharCode(65+i); sel.appendChild(o);}
    const ta = document.createElement('textarea'); ta.rows=4; ta.placeholder='Add or refine the explanation...';
    const saveBtn = document.createElement('button'); saveBtn.className='btn'; saveBtn.textContent='Save';
    if (expl[q.qnum]) { if (expl[q.qnum].correct !== undefined) sel.value = String(expl[q.qnum].correct); if (expl[q.qnum].text) ta.value = expl[q.qnum].text; }
    saveBtn.addEventListener('click', () => {
      const corr = sel.value === '' ? undefined : Number(sel.value);
      const txt = ta.value.trim();
      expl[q.qnum] = { correct: corr, text: txt };
      saveExpl(expl);
      alert('Saved!');
      // update just this card
      const labels = Array.from(document.getElementById('q'+q.qnum).querySelectorAll('.choices label'));
      labels.forEach((lab, i)=>{ lab.classList.remove('choice-ok','choice-bad'); if (corr !== undefined && i===corr) lab.classList.add('choice-ok'); });
    });
    edit.appendChild(small); edit.appendChild(sel); edit.appendChild(ta); edit.appendChild(saveBtn);
    card.appendChild(edit);

    const tools = document.createElement('div');
    tools.className='tools';
    const showExp = document.createElement('button'); showExp.className='btn'; showExp.textContent='üí° Show explanation';
    showExp.addEventListener('click', ()=>{ ex.style.display = ex.style.display === 'block' ? 'none' : 'block'; });
    const toggle = document.createElement('button'); toggle.className='btn'; toggle.textContent='üìù Edit answer';
    toggle.addEventListener('click', ()=>{ edit.style.display = edit.style.display === 'flex' ? 'none' : 'flex'; });
    tools.appendChild(showExp); tools.appendChild(toggle);
    card.appendChild(tools);

    root.appendChild(card);
  });
  updateStats();
}

function onSelect(q, chosen, labelEl){
  state[q.qnum] = chosen;
  saveState(state);
  const card = labelEl.closest('.qcard');
  const correct = (expl[q.qnum]?.correct ?? q.correct);
  const res = card.querySelector('.result');

  if (correct === undefined || correct === null){
    res.className = 'result';
    res.textContent = 'Saved. (No official answer set for this question. You can set it via üìù Edit.)';
  } else {
    const ok = Number(chosen) === Number(correct);
    res.className = 'result ' + (ok ? 'res-ok' : 'res-bad');
    res.textContent = ok ? '‚úÖ Correct!' : '‚ùå Try again.';
  }

  const ex = card.querySelector('.explain');
  ex.style.display = 'block';

  const labels = Array.from(card.querySelectorAll('.choices label'));
  labels.forEach((lab, i) => {
    lab.classList.remove('choice-ok','choice-bad');
    const corr = (expl[q.qnum]?.correct ?? q.correct);
    if (corr !== undefined && corr === i) lab.classList.add('choice-ok');
    if (Number(state[q.qnum]) === i && (corr === undefined || Number(state[q.qnum]) !== corr)) lab.classList.add('choice-bad');
  });

  updateStats();
}

function updateStats(){
  const answered = Object.keys(state).length;
  const total = DATA.length;
  document.getElementById('progress').textContent = `${answered} / ${total} answered`;
  let score = 0;
  DATA.forEach(q=>{
    const a = state[q.qnum];
    const corr = (expl[q.qnum]?.correct ?? q.correct);
    if (a !== undefined && corr !== undefined && Number(a) === Number(corr)) score++;
  });
  document.getElementById('score').textContent = `Score: ${score}`;
}

document.getElementById('showScore').addEventListener('click', updateStats);
document.getElementById('reset').addEventListener('click', () => {
  if (confirm('Clear all saved answers and overrides?')) {
    localStorage.removeItem(KEY_STORAGE);
    localStorage.removeItem(EXPL_STORAGE);
    location.reload();
  }
});
document.getElementById('export').addEventListener('click', () => {
  const blob = new Blob([JSON.stringify({answers:state, overrides:expl}, null, 2)], {type: 'application/json'});
  const url = URL.createObjectURL(blob);
  const a = document.createElement('a'); a.href = url; a.download = 'mcq_progress.json';
  document.body.appendChild(a); a.click(); a.remove();
  URL.revokeObjectURL(url);
});
document.getElementById('importBtn').addEventListener('click', ()=> document.getElementById('import').click());
document.getElementById('import').addEventListener('change', (ev)=>{
  const file = ev.target.files[0]; if (!file) return;
  const reader = new FileReader();
  reader.onload = ()=>{
    try {
      const obj = JSON.parse(reader.result);
      if (obj.answers) localStorage.setItem(KEY_STORAGE, JSON.stringify(obj.answers));
      if (obj.overrides) localStorage.setItem(EXPL_STORAGE, JSON.stringify(obj.overrides));
      alert('Imported!'); location.reload();
    } catch(e) { alert('Invalid file.'); }
  };
  reader.readAsText(file);
});

render();
</script>

<footer class="small" style="margin:24px 0 64px; text-align:center;">
  Built for touch (iPad Edge/Safari). Large tap targets, no hover-only UI, and state saved in localStorage.
</footer>
</body>
</html>